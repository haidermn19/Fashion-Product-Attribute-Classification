{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84705,"databundleVersionId":9755748,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-11T20:10:59.507320Z","iopub.execute_input":"2024-10-11T20:10:59.507967Z","iopub.status.idle":"2024-10-11T20:12:05.854300Z","shell.execute_reply.started":"2024-10-11T20:10:59.507918Z","shell.execute_reply":"2024-10-11T20:12:05.852922Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#see total no of images and rows in train.csv\n\n'''\nimport os\nimport pandas as pd\n\n# Path to your training images and CSV file\nimage_dir = '/kaggle/input/visual-taxonomy/train_images'\ncsv_file = '/kaggle/input/visual-taxonomy/train.csv'\n\n# Count the number of images\ntotal_images = len(os.listdir(image_dir))\n\n# Load the CSV file and count the number of rows\ntrain_df = pd.read_csv(csv_file)\ntotal_rows = len(train_df)\n\n# Display the results\nprint(f'Total number of images: {total_images}')\nprint(f'Total number of rows in train.csv: {total_rows}')\n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:34:48.805734Z","iopub.execute_input":"2024-10-14T12:34:48.806228Z","iopub.status.idle":"2024-10-14T12:34:50.988006Z","shell.execute_reply.started":"2024-10-14T12:34:48.806181Z","shell.execute_reply":"2024-10-14T12:34:50.986808Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Total number of images: 70379\nTotal number of rows in train.csv: 70213\n","output_type":"stream"}]},{"cell_type":"code","source":"#see what all images are missing\n\n'''import os\nimport pandas as pd\n\n# Path to your training images and CSV file\nimage_dir = '/kaggle/input/visual-taxonomy/train_images'\ncsv_file = '/kaggle/input/visual-taxonomy/train.csv'\n\n# Load the CSV file\ntrain_df = pd.read_csv(csv_file)\n\n# Get a list of image filenames based on the id, ensuring leading zeros\nimage_filenames = {f\"{int(row['id']):06}.jpg\" for _, row in train_df.iterrows()}\n\n# Get a list of image filenames from the directory\nexisting_filenames = set(os.listdir(image_dir))\n\n# Find images in the directory that are not in the CSV\nmissing_images = existing_filenames - image_filenames\n\n# Output the number of missing images and the list (if desired)\nprint(f'Number of missing images: {len(missing_images)}')\nprint('Missing images:', missing_images)\n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-13T08:14:00.959682Z","iopub.execute_input":"2024-10-13T08:14:00.960807Z","iopub.status.idle":"2024-10-13T08:14:00.970530Z","shell.execute_reply.started":"2024-10-13T08:14:00.960757Z","shell.execute_reply":"2024-10-13T08:14:00.969210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#delete unwanted directories in output\n\n'''\nimport shutil\n\n# Define the directories or files you want to delete\nunwanted_dir = '/kaggle/working/'  # Example of a folder to delete\n\n# Check if the directory exists, then delete\nif os.path.exists(unwanted_dir):\n    shutil.rmtree(unwanted_dir)  # Recursively delete the folder and its contents\n    print(f\"{unwanted_dir} has been deleted.\")\nelse:\n    print(f\"{unwanted_dir} does not exist.\")\n    \n    '''","metadata":{"execution":{"iopub.status.busy":"2024-10-13T14:39:26.514526Z","iopub.execute_input":"2024-10-13T14:39:26.515051Z","iopub.status.idle":"2024-10-13T14:39:26.844587Z","shell.execute_reply.started":"2024-10-13T14:39:26.515004Z","shell.execute_reply":"2024-10-13T14:39:26.842912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Data Preprocessing","metadata":{}},{"cell_type":"code","source":"#1.1 Load the Dataset\n'''\nimport pandas as pd\nimport os\nfrom PIL import Image\n\n# Load the CSV file containing the metadata (attributes)\ntrain_data = pd.read_csv('/kaggle/input/visual-taxonomy/train.csv')\n\n# Display the first few rows of the dataframe\nprint(train_data.head())\n# Print all column names\nprint(train_data.columns)\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1 Image Preprocessing","metadata":{}},{"cell_type":"code","source":"#2.1.1  image loading and organizing into subfolders\n'''\nimport os\nimport pandas as pd\nimport shutil\nfrom tqdm import tqdm\n\n# Load the train.csv to understand the category distribution\ntrain_df = pd.read_csv('/kaggle/input/visual-taxonomy/train.csv')\n\n# Define directories\nsource_dir = '/kaggle/input/visual-taxonomy/train_images/'\ndestination_dir = '/kaggle/working/organized_train_images/'\n\n# Create new directories based on categories\nif not os.path.exists(destination_dir):\n    os.makedirs(destination_dir)\n\n# Group images by category and organize them into subfolders\nfor category in train_df['Category'].unique():\n    category_folder = os.path.join(destination_dir, category)\n    if not os.path.exists(category_folder):\n        os.makedirs(category_folder)\n\n# Move images into respective subfolders by category\nfor index, row in tqdm(train_df.iterrows(), total=len(train_df)):\n    image_id = f\"{row['id']:06d}.jpg\"  # Zero-padded image ID\n    category = row['Category']\n    src_path = os.path.join(source_dir, image_id)\n    dest_path = os.path.join(destination_dir, category, image_id)\n\n    # Move the file\n    if os.path.exists(src_path):\n        shutil.copy(src_path, dest_path)  # You can use shutil.move if you want to remove the original files\n\nprint(\"Images organized into subfolders by category.\")\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2.1.2 Image Resizing with Aspect Ratio Preservation and Padding (with Subfolder Preservation)\n'''\nimport os\nfrom PIL import Image\n\n# Image size (target size after resizing with padding)\nimage_size = (512, 512)\n\n# Function to resize image while preserving aspect ratio and adding padding\ndef resize_with_padding(image, target_size=image_size):\n    # Resize the image while maintaining aspect ratio\n    img_w, img_h = image.size\n    target_w, target_h = target_size\n    \n    # Calculate the new size to preserve aspect ratio\n    ratio = min(target_w / img_w, target_h / img_h)\n    new_size = (int(img_w * ratio), int(img_h * ratio))\n    \n    # Resize the image\n    resized_image = image.resize(new_size, Image.LANCZOS)\n    \n    # Create a new image with the target size and a black background\n    new_image = Image.new(\"RGB\", target_size, (0, 0, 0))\n    \n    # Paste the resized image onto the center of the black canvas\n    new_image.paste(resized_image, ((target_w - new_size[0]) // 2, (target_h - new_size[1]) // 2))\n    \n    return new_image\n\n# Directory paths\norganized_dir = '/kaggle/working/organized_train_images/'  # Input directory with category subfolders\noutput_dir = '/kaggle/working/resized_images/'  # Output directory for resized images\nos.makedirs(output_dir, exist_ok=True)\n\n# Resize and save all images while preserving subfolder structure\nfor subdir, dirs, files in os.walk(organized_dir):\n    for file in files:\n        if file.endswith(('jpg', 'jpeg', 'png')):\n            image_path = os.path.join(subdir, file)\n\n            # Create corresponding subfolder structure in output directory\n            relative_subdir = os.path.relpath(subdir, organized_dir)  # Get relative path to preserve category subfolders\n            output_subdir = os.path.join(output_dir, relative_subdir)\n            os.makedirs(output_subdir, exist_ok=True)  # Ensure subfolders exist\n            \n            output_path = os.path.join(output_subdir, file)  # Save to the correct subfolder\n            \n            # Load image, resize with padding, and save\n            image = Image.open(image_path)\n            resized_image = resize_with_padding(image)\n            resized_image.save(output_path, 'JPEG')\n            \nprint(\"All images resized with padding and saved with folder structure preserved.\")\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AUGUMENTATION\n\nHigh severity (< 1000 samples):\n\n2 images with both basic and advanced augmentations\n1 MixUp image\n1 CutMix image\n1 original image (copied)\nTotal: 7 images (6 augmented + 1 original)\n\n\nMedium severity (1000-5000 samples):\n\n3 images with basic augmentations (50% chance of also getting advanced augmentations)\n1 original image (copied)\nTotal: 4 images (3 augmented + 1 original)\n\n\nLow severity (5000-10000 samples):\n\n1 image with basic augmentations\n1 original image (copied)\nTotal: 2 images (1 augmented + 1 original)\n\n\nVery high sample count (> 10000):\n\n20% chance of getting 1 image with basic augmentations\n1 original image (copied)\nTotal: 1-2 images (0-1 augmented + 1 original)","metadata":{}},{"cell_type":"code","source":"'''\nimport pandas as pd\n\n# Load train.csv\ntrain_df = pd.read_csv('/kaggle/input/visual-taxonomy/train.csv')\n\n# Define thresholds for augmentation\naugmentation_thresholds = {\n    \"low\": {\"limit\": 1000, \"augment_count\": 4},\n    \"medium\": {\"limit\": 5000, \"augment_count\": 3},\n    \"high\": {\"limit\": 10000, \"augment_count\": 1},\n    \"very_high\": {\"limit\": float('inf'), \"augment_count\": 0.2}, # Random 20% of the images\n}\n\n# Analyze class distribution for each attribute\nclass_distributions = {}\n\nfor i in range(1, 11):\n    attr_col = f'attr_{i}'\n    class_distributions[attr_col] = train_df[attr_col].value_counts().to_dict()\n\n# Summary of distributions for further augmentation strategy\nclass_distributions\n\n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-12T23:43:31.510172Z","iopub.execute_input":"2024-10-12T23:43:31.510611Z","iopub.status.idle":"2024-10-12T23:43:31.740232Z","shell.execute_reply.started":"2024-10-12T23:43:31.510570Z","shell.execute_reply":"2024-10-12T23:43:31.739105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nimport os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport albumentations as A\nfrom tqdm import tqdm\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\n# Define paths\ninput_dir = '/kaggle/input/augumented-images/resized_images'\noutput_dir = '/kaggle/working/augmented_images/'\ntrain_csv_path = '/kaggle/input/visual-taxonomy/train.csv'\n\n# Load and analyze the dataset\ndef analyze_dataset(csv_path):\n    df = pd.read_csv(csv_path)\n    attribute_stats = {}\n    for col in df.columns:\n        if col.startswith('attr_'):\n            attribute_stats[col] = df[col].value_counts().to_dict()\n    return df, attribute_stats\n\n# Define augmentation strategies\ndef get_basic_augmentation():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(p=0.7),\n        A.ShiftScaleRotate(p=0.5),\n    ])\n\ndef get_advanced_augmentation():\n    return A.Compose([\n        A.OneOf([\n            A.GaussNoise(p=0.5),\n            A.GaussianBlur(p=0.5),\n        ], p=0.5),\n        A.OneOf([\n            A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=None, p=0.5),  # Set alpha_affine to None\n            A.GridDistortion(p=0.5),\n            A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=0.5),\n        ], p=0.5),\n    ])\n\n\ndef mixup(image1, image2, alpha=0.2):\n    lam = np.random.beta(alpha, alpha)\n    mixed_image = (lam * image1 + (1 - lam) * image2).astype(np.uint8)\n    return mixed_image, lam\n\ndef cutmix(image1, image2):\n    h, w = image1.shape[:2]\n    center_x, center_y = np.random.randint(w), np.random.randint(h)\n    size = min(w, h) // 2\n    x1, y1 = max(0, center_x - size // 2), max(0, center_y - size // 2)\n    x2, y2 = min(w, center_x + size // 2), min(h, center_y + size // 2)\n    \n    image_cut = image1.copy()\n    image_cut[y1:y2, x1:x2] = image2[y1:y2, x1:x2]\n    return image_cut\n\ndef augment_image(image, category, attributes, stats):\n    augmented_images = []\n    base_filename = os.path.splitext(os.path.basename(image))[0]\n    \n    # Determine augmentation severity based on the rarest attribute, ignoring NaN values\n    min_count = float('inf')\n    for attr in attributes:\n        if attr in stats and pd.notna(attributes[attr]):\n            attr_value = attributes[attr]\n            if attr_value in stats[attr]:\n                min_count = min(min_count, stats[attr][attr_value])\n    \n    # If all attributes are NaN, use a default severity\n    if min_count == float('inf'):\n        min_count = 5000  # Choose medium severity as default\n    \n    basic_aug = get_basic_augmentation()\n    advanced_aug = get_advanced_augmentation()\n    \n    img = np.array(Image.open(image))\n\n    if min_count < 1000:  # High severity\n        num_augmentations = 4\n        for i in range(num_augmentations):\n            augmented = basic_aug(image=img)['image']\n            augmented = advanced_aug(image=augmented)['image']\n            aug_filename = f\"{base_filename}_high_{i}.jpg\"\n            augmented_images.append((augmented, aug_filename))\n        \n        # Apply MixUp and CutMix\n        if len(os.listdir(os.path.dirname(image))) > 1:\n            other_image = np.random.choice([f for f in os.listdir(os.path.dirname(image)) if f != os.path.basename(image)])\n            other_img = np.array(Image.open(os.path.join(os.path.dirname(image), other_image)))\n            \n            mixed_img, _ = mixup(img, other_img)\n            augmented_images.append((mixed_img, f\"{base_filename}_mixup.jpg\"))\n            \n            cut_img = cutmix(img, other_img)\n            augmented_images.append((cut_img, f\"{base_filename}_cutmix.jpg\"))\n    \n    elif min_count < 5000:  # Medium severity\n        num_augmentations = 3\n        for i in range(num_augmentations):\n            augmented = basic_aug(image=img)['image']\n            if np.random.random() < 0.5:  # 50% chance of advanced aug\n                augmented = advanced_aug(image=augmented)['image']\n            aug_filename = f\"{base_filename}_medium_{i}.jpg\"\n            augmented_images.append((augmented, aug_filename))\n    \n    elif min_count < 10000:  # Low severity\n        augmented = basic_aug(image=img)['image']\n        aug_filename = f\"{base_filename}_low_0.jpg\"\n        augmented_images.append((augmented, aug_filename))\n    \n    else:  # Very high sample count\n        if np.random.random() < 0.2:  # 20% chance for images with >10000 samples\n            augmented = basic_aug(image=img)['image']\n            aug_filename = f\"{base_filename}_verylow_0.jpg\"\n            augmented_images.append((augmented, aug_filename))\n\n    return augmented_images\n\ndef augment_dataset(input_dir, output_dir, df, stats):\n    for category in os.listdir(input_dir):\n        category_dir = os.path.join(input_dir, category)\n        output_category_dir = os.path.join(output_dir, category)\n        os.makedirs(output_category_dir, exist_ok=True)\n\n        for image_name in tqdm(os.listdir(category_dir), desc=f\"Augmenting {category}\"):\n            image_path = os.path.join(category_dir, image_name)\n            image_id = int(os.path.splitext(image_name)[0])\n            \n            # Get attributes for this image\n            image_data = df[df['id'] == image_id].iloc[0]\n            attributes = {col: image_data[col] for col in df.columns if col.startswith('attr_')}\n\n            augmented_images = augment_image(image_path, category, attributes, stats)\n\n            for aug_img, aug_filename in augmented_images:\n                output_path = os.path.join(output_category_dir, aug_filename)\n                Image.fromarray(aug_img).save(output_path)\n\n            # Copy original image\n            shutil.copy(image_path, os.path.join(output_category_dir, image_name))\n\ndef update_train_csv(input_csv, output_dir, output_csv):\n    df = pd.read_csv(input_csv)\n    new_rows = []\n\n    for category in os.listdir(output_dir):\n        category_dir = os.path.join(output_dir, category)\n        for image_name in os.listdir(category_dir):\n            base_name, _ = os.path.splitext(image_name)\n            original_id = int(base_name.split('_')[0])\n            \n            original_row = df[df['id'] == original_id].iloc[0]\n            new_row = original_row.copy()\n            new_row['id'] = base_name\n            new_rows.append(new_row)\n\n    updated_df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n    updated_df.to_csv(output_csv, index=False)\n\n# Main execution\nif __name__ == \"__main__\":\n    print(\"Starting data augmentation process...\")\n    \n    print(\"Analyzing dataset...\")\n    df, stats = analyze_dataset(train_csv_path)\n    \n    print(\"Augmenting images...\")\n    augment_dataset(input_dir, output_dir, df, stats)\n    \n    print(\"Updating train CSV...\")\n    update_train_csv(train_csv_path, output_dir, '/kaggle/working/updated_train.csv')\n    \n    print(\"Augmentation completed. Updated train CSV created.\")\n    print(f\"Augmented images are saved in: {output_dir}\")\n    print(\"Updated CSV file: /kaggle/working/updated_train.csv\")\n    \n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-12T23:44:00.116579Z","iopub.execute_input":"2024-10-12T23:44:00.117201Z","iopub.status.idle":"2024-10-12T23:44:21.321547Z","shell.execute_reply.started":"2024-10-12T23:44:00.117140Z","shell.execute_reply":"2024-10-12T23:44:21.319792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Create a subdirectory\nos.makedirs('/kaggle/working/my_dataset/', exist_ok=True)\n\n# Move the file to the new directory\nshutil.move('/kaggle/working/updated_train.csv', '/kaggle/working/my_dataset/updated_train.csv')\n\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nimport os\nimport pandas as pd\n\n# Path to your training images and CSV file\nimage_dir = '/kaggle/input/augumented-images/augmented_images'\ncsv_file = '/kaggle/input/augumented-images/final_preprocessed_images/updated_train_with_preprocessed.csv'\n\n# Count the total number of images in the direct subfolders\ntotal_images = 0\nfor subfolder in os.listdir(image_dir):\n    subfolder_path = os.path.join(image_dir, subfolder)\n    if os.path.isdir(subfolder_path):\n        total_images += len([f for f in os.listdir(subfolder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))])\n\n# Load the CSV file and count the number of rows\ntrain_df = pd.read_csv(csv_file)\ntotal_rows = len(train_df)\n\n# Display the results\nprint(f'Total number of images: {total_images}')\nprint(f'Total number of rows in train.csv: {total_rows}')\n\n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-13T08:20:57.865059Z","iopub.execute_input":"2024-10-13T08:20:57.866159Z","iopub.status.idle":"2024-10-13T08:20:59.511309Z","shell.execute_reply.started":"2024-10-13T08:20:57.866111Z","shell.execute_reply":"2024-10-13T08:20:59.510109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2. Check how many images are organized in subfolders\n'''\nfor root, dirs, files in os.walk('/kaggle/input/augumented-images/augmented_images'):\n    for dir_name in dirs:\n        dir_path = os.path.join(root, dir_name)\n        num_files = len(os.listdir(dir_path))\n        print(f\"Category '{dir_name}' contains {num_files} images.\")\n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-13T08:17:18.779253Z","iopub.execute_input":"2024-10-13T08:17:18.779656Z","iopub.status.idle":"2024-10-13T08:17:47.591635Z","shell.execute_reply.started":"2024-10-13T08:17:18.779617Z","shell.execute_reply":"2024-10-13T08:17:47.589641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3. Visualize some of the augmented images\n'''\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport random\n\n# Get a list of all image paths\nall_image_paths = []\nfor root, dirs, files in os.walk('/kaggle/input/augumented-images/augmented_images'):\n    for file in files:\n        if file.endswith(('jpg', 'jpeg', 'png')):\n            all_image_paths.append(os.path.join(root, file))\n\n# Randomly sample a few images to visualize\nsample_images = random.sample(all_image_paths, 5)\n\n# Plotting the images\nplt.figure(figsize=(10, 10))\nfor i, image_path in enumerate(sample_images):\n    img = Image.open(image_path)\n    plt.subplot(1, 5, i+1)\n    plt.imshow(img)\n    plt.axis('off')\nplt.show()\n\n'''\n","metadata":{"execution":{"iopub.status.busy":"2024-10-12T22:43:50.796361Z","iopub.execute_input":"2024-10-12T22:43:50.796861Z","iopub.status.idle":"2024-10-12T22:45:47.659310Z","shell.execute_reply.started":"2024-10-12T22:43:50.796817Z","shell.execute_reply":"2024-10-12T22:45:47.658139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load the new Dataset updated_train.csv\n'''\nimport pandas as pd\nimport os\nfrom PIL import Image\n\n# Load the CSV file containing the metadata (attributes)\ntrain_data = pd.read_csv('/kaggle/input/augumented-images/my_dataset/updated_train.csv')\n\n# Display the first few rows of the dataframe\nprint(train_data.head(20))\n# Print all column names\nprint(train_data.columns)\n\n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-12T22:32:08.496285Z","iopub.execute_input":"2024-10-12T22:32:08.496691Z","iopub.status.idle":"2024-10-12T22:32:11.092126Z","shell.execute_reply.started":"2024-10-12T22:32:08.496649Z","shell.execute_reply":"2024-10-12T22:32:11.090883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Attribute Distribution in new dataset\n\n'''\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# List of attributes to check the distribution for\nattributes = ['attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']  \n\nfor attr in attributes:\n    # Plot the distribution of each attribute\n    plt.figure(figsize=(10, 6))\n    sns.countplot(y=train_data[attr], order=train_data[attr].value_counts().index)\n    plt.title(f'Distribution of {attr}')\n    plt.show()\n    \n    # Print the distribution of each attribute\n    print(f\"Distribution of {attr}:\")\n    attr_distribution = train_data[attr].value_counts()\n    for value, count in attr_distribution.items():\n        print(f\"{value}: {count}\")\n    print(\"\\n\")\n    \n    '''","metadata":{"execution":{"iopub.status.busy":"2024-10-13T08:15:28.216013Z","iopub.execute_input":"2024-10-13T08:15:28.216409Z","iopub.status.idle":"2024-10-13T08:15:28.225089Z","shell.execute_reply.started":"2024-10-13T08:15:28.216368Z","shell.execute_reply":"2024-10-13T08:15:28.224029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Code for Image Normalization using ImageNet Mean-Std for working well with swim along with image normalization, quality enhancement, deduplication, and optimization","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\n# Define paths\nINPUT_DIR = '/kaggle/input/augumented-images/augmented_images'\nOUTPUT_DIR = '/kaggle/working/Final_images_for_model/'\nTRAIN_CSV_PATH = '/kaggle/input/augumented-images/final_preprocessed_images/updated_train_with_preprocessed.csv'\nOUTPUT_CSV_FILE = '/kaggle/working/Final_dataset_for_model.csv'  # Changed to .csv file\n\n# Ensure output directories exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(os.path.dirname(OUTPUT_CSV_FILE), exist_ok=True)\n\n# Define the transform\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nclass FashionDataset(Dataset):\n    def __init__(self, csv_file, image_dir, transform=None):\n        self.data = pd.read_csv(csv_file)\n        self.image_dir = image_dir\n        self.transform = transform\n        self.image_files = self._get_image_files()\n\n    def _get_image_files(self):\n        image_files = []\n        for category in os.listdir(self.image_dir):\n            category_path = os.path.join(self.image_dir, category)\n            if os.path.isdir(category_path):\n                for img_file in os.listdir(category_path):\n                    image_files.append((category, img_file))\n        return image_files\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        category, img_file = self.image_files[idx]\n        img_path = os.path.join(self.image_dir, category, img_file)\n        image = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        original_id = img_file.split('_')[0].split('.')[0]\n        \n        try:\n            csv_row = self.data[self.data['id'].astype(str) == original_id].iloc[0]\n            csv_data = csv_row.to_dict()\n            \n            return {\n                'image': image,\n                'img_path': img_path,\n                'csv_data': csv_data\n            }\n        except IndexError:\n            print(f\"Warning: No matching CSV entry found for image {img_file}\")\n            return None\n\ndef custom_collate(batch):\n    batch = list(filter(lambda x: x is not None, batch))\n    if len(batch) == 0:\n        return None\n    return {\n        'image': torch.stack([item['image'] for item in batch if isinstance(item, dict) and 'image' in item]),\n        'img_path': [item['img_path'] for item in batch if isinstance(item, dict) and 'img_path' in item],\n        'csv_data': [item['csv_data'] for item in batch if isinstance(item, dict) and 'csv_data' in item]\n    }\n\ndef save_processed_images(images, img_paths):\n    new_paths = []\n    for img, old_path in zip(images, img_paths):\n        img = transforms.ToPILImage()(img.cpu())\n        \n        category = os.path.dirname(old_path).split('/')[-1]\n        category_dir = os.path.join(OUTPUT_DIR, category)\n        os.makedirs(category_dir, exist_ok=True)\n        \n        new_filename = os.path.basename(old_path)\n        output_path = os.path.join(category_dir, new_filename)\n        img.save(output_path)\n        new_paths.append(output_path)\n    \n    return new_paths\n\nBATCH_SIZE = 32\ndataset = FashionDataset(csv_file=TRAIN_CSV_PATH, image_dir=INPUT_DIR, transform=transform)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, collate_fn=custom_collate)\n\nprint(\"Processing images and preparing new CSV...\")\nnew_data = []\nfor batch in tqdm(dataloader):\n    if batch is None:\n        continue\n    images, img_paths, csv_data_list = batch['image'], batch['img_path'], batch['csv_data']\n    new_paths = save_processed_images(images, img_paths)\n    for new_path, csv_data in zip(new_paths, csv_data_list):\n        new_row = csv_data.copy()\n        new_row['image_path'] = new_path\n        new_row['augmentation_type'] = os.path.basename(new_path).split('_')[-1].split('.')[0]\n        new_data.append(new_row)\n\nnew_df = pd.DataFrame(new_data)\nnew_df.to_csv(OUTPUT_CSV_FILE, index=False)\n\nprint(f\"Normalized and enhanced images saved to {OUTPUT_DIR}\")\nprint(f\"New CSV file with updated image paths saved to {OUTPUT_CSV_FILE}\")\n\ntotal_processed = sum([len(files) for r, d, files in os.walk(OUTPUT_DIR)])\ncsv_entries = len(new_df)\n\nprint(f\"Total processed images: {total_processed}\")\nprint(f\"Total CSV entries: {csv_entries}\")\n\nif total_processed == csv_entries:\n    print(\"Number of processed images matches CSV entries. Data is ready for model input.\")\nelse:\n    print(\"Warning: Mismatch between processed images and CSV entries. Please check the data.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T14:41:13.417340Z","iopub.execute_input":"2024-10-13T14:41:13.417792Z","iopub.status.idle":"2024-10-13T14:41:42.923204Z","shell.execute_reply.started":"2024-10-13T14:41:13.417742Z","shell.execute_reply":"2024-10-13T14:41:42.921107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}